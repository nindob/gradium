# Gradium 
A neural network framework from scratch in C++ to understand the fundamentals of automatic differentiation and computational graphs.

- [Current] Micrograd Scalar Backprop and Layers with Toposort
- [Next Steps]
    - Vector & Matrix Support
    - Operator Fusion
    - Computational Graph Optimization

To Run:
g++ -std=c++20 main.cpp prime.cpp loss.cpp -o gradium
./gradium